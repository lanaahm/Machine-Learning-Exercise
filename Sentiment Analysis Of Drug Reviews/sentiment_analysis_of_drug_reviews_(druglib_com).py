# -*- coding: utf-8 -*-
"""Sentiment_Analysis_of_Drug_Reviews_(Druglib_com).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14fmhzeY0Ty0_6W1m57M_rrAGq5zzNcJ8

# Drug Classfication Based on Sentiment Analysis Review (Druglib.com)
## Case Study:

The number of medicinal products circulating in pharmacies causes consumers to compare the product with other products. The machine learning model that is built is expected to be able to provide product classfication based on the types of complaints experienced by consumers or the products they want to find. Drug classfication are obtained from a collection of reviews related to preferences and conditions, effectiveness, reviews, and product ratings from review data on Druglib.com. The drug review will then be analyzed further using sentiment analysis with an algorithm model from long short-term memory (LSTM).

## Data Set Information:

The dataset provides patient reviews on specific drugs along with related conditions. Furthermore, reviews are grouped into reports on the three aspects benefits, side effects and overall comment. Additionally, ratings are available concerning overall satisfaction as well as a 5 step side effect rating and a 5 step effectiveness rating. The data was obtained by crawling online pharmaceutical review sites. The intention was to study

1. sentiment analysis of drug experience over multiple facets, i.e. sentiments learned on specific aspects such as effectiveness and side effects,
2. the transferability of models among domains, i.e. conditions, and
3. the transferability of models among different data sources (see 'Drug Review Dataset (Drugs.com)').

The data is split into a train (75%) a test (25%) partition (see publication) and stored in two .tsv (tab-separated-values) files, respectively.

## Informasi Atribut:

1. urlNama Obat (kategoris): nama obat
2. kondisi (kategoris): nama kondisi
3. manfaatReview (teks): pasien tentang manfaat
4. Efek sampingReview (teks): pasien tentang efek samping
5. komentarReview (teks): pasien secara keseluruhan komentar
6. peringkat (numerik): peringkat pasien bintang 10
7. Efek samping (kategoris): peringkat efek samping 5 langkah
8. efektivitas (kategoris): peringkat efektivitas 5 langkah

Source dataset: [Drug Review Dataset (Druglib.com)](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Druglib.com%29)

#### Import Library
"""

import tensorflow as tf 
print(tf.__version__)

device_name = tf.test.gpu_device_name()
if "GPU" not in device_name:
    print("GPU device not found")
print('Found GPU at: {}'.format(device_name))

# Commented out IPython magic to ensure Python compatibility.
import os
import zipfile
import wordcloud as ws
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

from tensorflow import keras
from sklearn import preprocessing
from textblob import Word
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from keras.callbacks import EarlyStopping

import nltk
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.corpus import stopwords

"""#### Import Dataset"""

!wget --no-check-certificate \
  https://archive.ics.uci.edu/ml/machine-learning-databases/00461/drugLib_raw.zip \
  -O /tmp/drugLib_raw.zip

local_zip = '/tmp/drugLib_raw.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp/drugLib_raw')
zip_ref.close()
base_dir = '/tmp/drugLib_raw/'

"""#### Import pre-trained word embeddings"""

!wget --no-check-certificate \
  http://nlp.stanford.edu/data/glove.6B.zip \
  -O /tmp/glove.6B.zip

local_zip = '/tmp/glove.6B.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp/glove')
zip_ref.close()
pretrained_dir = '/tmp/glove/'

"""#### Exploring Data

##### Convert Dataset into pandas
"""

# Import dataset train and test to pandas
df_train = pd.read_csv(os.path.join(base_dir, 'drugLibTrain_raw.tsv'), sep='\t')
df_test = pd.read_csv(os.path.join(base_dir, 'drugLibTest_raw.tsv'), sep='\t')

# merge dataset train and test
df_all = pd.concat([df_train.iloc[:, 1:], df_test.iloc[:, 1:]], sort=True).reset_index(drop=True)

display(df_all.head())
print('Dataset Shape : {}'.format(df_all.shape))
print('Dataset columns : {}'.format(df_all.columns.values))

"""##### Check Missing Value"""

df_all.isnull().sum()

df_all.loc[df_all.isnull().any(axis=1)]

"""##### Fill in Missing Value
In the Condition column included in the keflex product, fill in the infection value, as this product is frequently used for infection, and data containing other missing values will be removed because the data are owned by different reviewers

"""

df_all['condition'] = df_all['condition'].fillna('infection')
df_all.dropna(axis=0, subset=['commentsReview', 'sideEffectsReview'], inplace=True)
df_all.info(memory_usage=False)

"""##### Data Visualization"""

rating = df_all['rating'].value_counts()
rating.plot(kind='bar', figsize=(14,6))
plt.xlabel('', fontsize=20)
plt.ylabel('', fontsize=20)
plt.title('Count values of rating', fontsize=20)

"""#### Feature Engineering

##### Stopword Remove
"""

english_stops = set(stopwords.words('english'))

#merge columns review
df_all['review'] = df_all['condition'] + df_all['commentsReview'] + df_all['benefitsReview'] + df_all['sideEffectsReview'] + df_all['sideEffects']
df_all['review'] = df_all['review'].replace({'<.*?>': ''}, regex = True) # remove html tag
df_all['review'] = df_all['review'].replace({'[^A-Za-z]': ' '}, regex = True) # remove non alphabet
df_all['review'] = df_all['review'].replace({'\s+[a-zA-Z]\s+': ' '}, regex = True) # Single character removal
df_all['review'] = df_all['review'].replace({'\s+': ' '}, regex = True) # Replacing the digits/numbers

df_all['review'] = df_all['review'].apply(lambda review: [w for w in review.split() if w not in english_stops]) # remove stop words
df_all['review'] = df_all['review'].apply(lambda review: [Word(w).lemmatize() for w in review]) # lemmatize
df_all['review'] = df_all['review'].apply(lambda review: [w.lower() for w in review]) # lower case

common_words=''
for x,i in enumerate(df_all.review):
    common_words += ' '.join(i)+' '
wordcloud = ws.WordCloud().generate(common_words)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""##### Encoding"""

# encode rating
df_all['rating_reviews'] = df_all['rating'].apply(lambda x: 'netral' if x > 3 and x < 6 else 'negatif' if x < 3 else 'positif')
encode = pd.get_dummies(df_all['rating_reviews'])

dfs = pd.concat([df_all, encode], axis=1)
dfs = dfs.drop(columns=['commentsReview', 'benefitsReview', 'sideEffectsReview'])

feature = dfs['review'].values
target = dfs[encode.columns].values

"""##### Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(feature, target, test_size=0.2)

def get_max_length():
    review_length = []
    for review in X_train:
        review_length.append(len(review))

    return int(np.ceil(np.mean(review_length)))

"""##### Tokenizer"""

tokenizer = Tokenizer(lower=False, oov_token='-')
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)
 
sekuens_train = tokenizer.texts_to_sequences(X_train)
sekuens_test = tokenizer.texts_to_sequences(X_test)

max_length = get_max_length()

padded_train = pad_sequences(sekuens_train, maxlen=max_length, padding='post', truncating='post')
padded_test = pad_sequences(sekuens_test, maxlen=max_length, padding='post', truncating='post')

total_words = len(tokenizer.word_index) + 1

"""##### pre-trained word embeddings"""

path_to_glove_file = os.path.join(
    pretrained_dir, 'glove.6B.100d.txt'
)

embeddings_index = dict()
with open(path_to_glove_file) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))

embedding_matrix = np.zeros((total_words, 100))

for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

"""#### Build Model"""

def plot(val_train, val_test, title):
  plt.plot(val_train)
  plt.plot(val_test)
  plt.title(title)
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['Train', 'Test'], loc='upper right')
  plt.show()

"""###### GridSearchCV"""

from keras import backend as backend
def create_model(unit=128, LSTM_OUT=64, optimizer='adam'):
    backend.clear_session()
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Embedding(total_words, EMBED_DIM, weights=[embedding_matrix], input_length=max_length, trainable=False))
    model.add(tf.keras.layers.SpatialDropout1D(0.2))
    model.add(tf.keras.layers.LSTM(LSTM_OUT, return_sequences=True))
    model.add(tf.keras.layers.Dense(unit, activation='relu'))
    model.add(tf.keras.layers.LSTM(LSTM_OUT))
    model.add(tf.keras.layers.Dense(unit, activation='relu'))
    model.add(tf.keras.layers.Dense(y_train.shape[1], activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model, batch_size=64, verbose=1)

params = {
    'optimizer': ['Adam', 'sgd'],
    }
grid_result = GridSearchCV(estimator=model, param_grid=params, cv=3)
grid_result = grid_result.fit(padded_train, y_train)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

EMBED_DIM = 100
LSTM_OUT = 64

num_epochs = 100
num_batch_size = 64
callbacks = EarlyStopping(monitor='val_accuracy', 
                   mode='max', 
                   patience=5, 
                   restore_best_weights=True)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(total_words, EMBED_DIM, weights=[embedding_matrix], input_length=max_length, trainable=False),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')
])
model.summary()
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

history1 = model.fit(padded_train, y_train, batch_size=num_batch_size, epochs=100, 
                    validation_data=(padded_test, y_test), verbose=1, callbacks=[callbacks])

"""###### Plot Accuracy"""

plot(history1.history['accuracy'], history1.history['val_accuracy'], 'Model-1 accuracy')

"""###### Plot Loss"""

plot(history1.history['loss'], history1.history['val_loss'], 'Model-1 loss')

"""###### Model evaluate score"""

score = model.evaluate(padded_test, y_test, verbose=0)
print('Test loss:', score[0]) 
print('Test accuracy:', score[1])