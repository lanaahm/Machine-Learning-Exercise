# -*- coding: utf-8 -*-
"""Fashion_Product_Images_(Classification).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12T_OSTPlkKmyJE-t8xtrpyo0NsuGfkOn

# Fashion Product Images (Classification)

---

**Disclaimer** :

The code below was created to complete of Dicoding course

## Data Set Information:
![](https://storage.googleapis.com/kaggle-datasets-images/139630/329006/720cd7ceb25eb130d0b873464f734370/dataset-cover.png?t=2019-03-15-02-52-57)
Thr growing e-commerce industry presents us with a large dataset waiting to be scraped and researched upon. In addition to professionally shot high resolution product images, we also have multiple label attributes describing the product which was manually entered while cataloging. To add to this, we also have descriptive text that comments on the product characteristics.

## Content
Each product is identified by an ID like 42431. You will find a map to all the products in styles.csv. From here, you can fetch the image for this product from images/42431.jpg and the complete metadata from styles/42431.json.

To get started easily, we also have exposed some of the key product categories and it's display name in styles.csv.

Source dataset: [Fashion Product Images Dataset
](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset)

#### Import Dataset
"""

import os
os.environ['KAGGLE_USERNAME'] = 'lanaahm'
os.environ['KAGGLE_KEY'] = 'c5dac849baf021db5db739062fdf3105'

!kaggle datasets download -d paramaggarwal/fashion-product-images-dataset

!unzip /content/fashion-product-images-dataset.zip

BASE_PATH = '/content/fashion-dataset/fashion-dataset/'
IMG_PATH  = BASE_PATH + 'images/'

"""#### Import Library"""

import os
import random
import time
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from math import ceil

"""#### Exploring Data
##### Showing dataset with pandas
"""

df = pd.read_csv(BASE_PATH + 'styles.csv', error_bad_lines=False, warn_bad_lines=False)
df.columns = df.columns.str.lower()
df['image'] = df['id'].map(lambda x: str(x) + '.jpg')

display(df.head())
display(df.shape)

"""##### Count products for each category"""

dfarticles = df.groupby('subcategory', as_index=False)['id'].count().sort_values(by=['id'], ascending=True)
dfarticles

"""##### Visualization product each category"""

plt.figure(figsize=(8,13))
plt.barh(dfarticles['subcategory'].loc[dfarticles['id'] > 500], dfarticles['id'].loc[dfarticles['id'] > 500])
plt.title('Count data for each category with a value of more than 500')
plt.xlabel('Total')
plt.ylabel('Labels')
plt.show()

"""##### Check random product"""

imglist = [IMG_PATH + x for x in df['image'].sample(10).values]

fig, ax = plt.subplots(2,5, figsize=(18,10))
for index, img_file in enumerate(imglist):
  img = plt.imread(img_file)
  height, width, depth = img.shape
  figsize = width / float(80), height / float(80)
  x = int(index / 5)
  y = index % 5
  ax[x,y].imshow(img)
  ax[x,y].set_title('Image '+ str(index+1) +' (random)')
  fig.set_figwidth(figsize[0])
plt.show()

"""#### Data Preparation
##### Image Augmentation
"""

train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale = 1./255, #Normalization
    horizontal_flip=True, #Flip horizontally
    validation_split = 0.2 #Split data 80% train 20% val
)

test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255, #Normalization
    validation_split = 0.2 #Split data 80% train 20% val
)

img_shape = (224, 224, 3) #Based on resnet recommendation

training_generator = train_datagen.flow_from_dataframe(
    dataframe=df,
    directory=IMG_PATH,
    x_col="image",
    y_col="subcategory",
    shuffle=True,
    target_size= img_shape[:2],
    class_mode='categorical',
    batch_size=100,
    subset="training"
)

testing_generator = train_datagen.flow_from_dataframe(
    dataframe=df,
    directory=IMG_PATH,
    x_col="image",
    y_col="subcategory",
    shuffle=True,
    target_size= img_shape[:2],
    class_mode='categorical',
    batch_size=100,
    subset='validation'
)

"""#### Build Model"""

model = tf.keras.models.Sequential([
                                    tf.keras.applications.ResNet50V2(input_shape=img_shape, include_top=False),
                                    tf.keras.layers.Conv2D(256, (4,4), activation='relu'),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    # Using transfer learning from ResNet50v2
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(128, activation='relu'),
                                    tf.keras.layers.Dense(len(training_generator.class_indices), activation='softmax')
])
model.summary()

def plot(val_train, val_test, title):
  plt.plot(val_train)
  plt.plot(val_test)
  plt.title(title)
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['Train', 'Test'], loc='upper right')
  plt.show()

#Custom callback
class myCallback(tf.keras.callbacks.Callback):
  def __init__(self, patience=0):
    super(myCallback, self).__init__()
    self.patience = patience
    self.best_weights = None
  def on_train_begin(self, logs=None):
    # The number of epoch it has waited when loss is no longer minimum.
    self.wait = 0
    # The epoch the training stops at.
    self.stopped_epoch = 0
    # Initialize the best as infinity.
    self.best_loss = np.Inf
    self.best_v_loss = np.Inf
  def on_epoch_end(self, epoch, logs=None):
    loss = 0 if logs.get('accuracy') is None else logs.get('accuracy')
    v_loss = 0 if logs.get('val_accuracy') is None else logs.get('val_accuracy')
    if np.less(loss, self.best_loss) and np.less(v_loss, self.best_v_loss):
      self.best_loss = loss
      self.best_v_loss = v_loss
      self.wait = 0
      self.best_weights = self.model.get_weights()
    else:
      self.wait += 1
      if(logs.get('accuracy') > 0.94 and logs.get('val_accuracy') >= 0.94):
        self.stopped_epoch = epoch
        self.model.stop_training = True
        print(f'\nAccuracy value has reached the requirement {self.best_v_loss}')
        self.model.set_weights(self.best_weights)
callbacks = myCallback(patience=5)

#Learning rate scheduler
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-4,
    decay_steps=10000,
    decay_rate=0.9)
optimizer = tf.keras.optimizers.Adamax(learning_rate=lr_schedule)

#Tensorboard
!rm -rf /content/logs
log_dir="/content/logs/"
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

history = model.fit(training_generator, 
                    epochs=100, 
                    steps_per_epoch = 30, 
                    validation_data=testing_generator, 
                    validation_steps= 10, 
                    callbacks=[callbacks, tensorboard_callback],
                    batch_size=256,
                    verbose=1)

"""###### Plot Accuracy"""

plot(history.history['accuracy'], history.history['val_accuracy'], 'Model Accuracy')

"""###### Plot Loss"""

plot(history.history['loss'], history.history['val_loss'], 'Model loss')

"""###### Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /content/logs

"""##### Save Model"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)

from google.colab import files
files.download('/content/model.tflite')

